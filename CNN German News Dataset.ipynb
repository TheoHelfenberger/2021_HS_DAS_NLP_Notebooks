{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fhnw-nlp-utils>=0.1.3 in /usr/local/lib/python3.6/dist-packages (0.1.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (3.3.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (5.8.0)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (3.2)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (4.0.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (1.19.5)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (1.8.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (1.1.5)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (3.6.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (0.24.2)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from fhnw-nlp-utils>=0.1.3) (0.70.12.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (8.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fhnw-nlp-utils>=0.1.3) (2.4.7)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (4.62.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (3.3.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->fhnw-nlp-utils>=0.1.3) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fhnw-nlp-utils>=0.1.3) (2021.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.3) (2021.10.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.3) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk->fhnw-nlp-utils>=0.1.3) (8.0.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.3) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->fhnw-nlp-utils>=0.1.3) (3.0.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /usr/local/lib/python3.6/dist-packages (from multiprocess->fhnw-nlp-utils>=0.1.3) (0.3.4)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (2021.5.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.12.0->gdown->fhnw-nlp-utils>=0.1.3) (1.7.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from click->nltk->fhnw-nlp-utils>=0.1.3) (4.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk->fhnw-nlp-utils>=0.1.3) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->click->nltk->fhnw-nlp-utils>=0.1.3) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-5.0.0-cp36-cp36m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.6 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastparquet\n",
      "  Downloading fastparquet-0.7.1-cp36-cp36m-manylinux2010_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.2)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.19.5)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from fastparquet) (1.1.5)\n",
      "Collecting thrift>=0.11.0\n",
      "  Downloading thrift-0.15.0.tar.gz (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 8.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting cramjam>=2.3.0\n",
      "  Downloading cramjam-2.4.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 28.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (57.4.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.1.0->fastparquet) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.1.0->fastparquet) (2.8.2)\n",
      "Requirement already satisfied: six>=1.7.2 in /usr/local/lib/python3.6/dist-packages (from thrift>=0.11.0->fastparquet) (1.15.0)\n",
      "Building wheels for collected packages: thrift\n",
      "  Building wheel for thrift (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for thrift: filename=thrift-0.15.0-cp36-cp36m-linux_x86_64.whl size=345208 sha256=d82877d9c7554299bf08b212b0b333a0e0758b0af71213f0baf555f4a1aea7b5\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/91/a0/aea72c790bd97f9c2f31ac5ca0b20b25992010a31e4357f50b\n",
      "Successfully built thrift\n",
      "Installing collected packages: pyarrow, thrift, cramjam, fsspec, fastparquet\n",
      "Successfully installed cramjam-2.4.0 fastparquet-0.7.1 fsspec-2021.10.1 pyarrow-5.0.0 thrift-0.15.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Tensorflow version: 2.6.0\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "!pip install 'fhnw-nlp-utils>=0.1.3'\n",
    "!pip install pyarrow fastparquet fasttext \n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "from fhnw.nlp.utils.storage import download\n",
    "from fhnw.nlp.utils.colab import runs_on_colab\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)\n",
    "\n",
    "#physical_devices = tf.config.list_physical_devices('GPU') \n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/german_news_articles_original_train_and_test_tokenized.parq\"\n",
    "data_all = load_dataframe(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_orig = data_all.loc[(data_all[\"split\"] == \"train\")]\n",
    "data_test_orig = data_all.loc[(data_all[\"split\"] == \"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'foo', b'bar', b'baz']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n",
    "list(text_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 2, 4],\n",
       "       [1, 3, 0, 0],\n",
       "       [2, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n",
    "max_features = 5000  # Maximum vocab size.\n",
    "max_len = 4  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len)\n",
    "\n",
    "# Now that the vocab layer has been created, call `adapt` on the text-only\n",
    "# dataset to create the vocabulary. You don't have to batch, but for large\n",
    "# datasets this means we're not keeping spare copies of the dataset.\n",
    "vectorize_layer.adapt(text_dataset.batch(64))\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "model.add(vectorize_layer)\n",
    "\n",
    "# Now, the model can map strings to integers, and you can add an embedding\n",
    "# layer to map these integers to learned embeddings.\n",
    "input_data = [[\"foo qux foo bar\"], [\"qux baz\"], ['foo']]\n",
    "model.predict(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'earth', 'wind', 'and', 'fire']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n",
    "max_len = 4  # Sequence length to pad the outputs to.\n",
    "\n",
    "# Create the layer, passing the vocab directly. You can also pass the\n",
    "# vocabulary arg a path to a file containing one vocabulary word per\n",
    "# line.\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len,\n",
    " vocabulary=vocab_data)\n",
    "\n",
    "# Create the model that uses the vectorize text layer\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Start by creating an explicit input layer. It needs to have a shape of\n",
    "# (1,) (because we need to guarantee that there is exactly one string\n",
    "# input per batch), and the dtype needs to be 'string'.\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "# The first layer in our model is the vectorization layer. After this\n",
    "# layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
    "# indices.\n",
    "model.add(vectorize_layer)\n",
    "\n",
    "# Because we've passed the vocabulary directly, we don't need to adapt\n",
    "# the layer - the vocabulary is already set. The vocabulary contains the\n",
    "# padding token ('') and OOV token ('[UNK]') as well as the passed tokens.\n",
    "vectorize_layer.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 1, 1],\n",
       "       [5, 1, 0, 0],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, the model can map strings to integers, and you can add an embedding\n",
    "# layer to map these integers to learned embeddings.\n",
    "input_data = [[\"earth wind among other things fire\"], [\"fire fighter\"], ['foo']]\n",
    "model.predict(input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
