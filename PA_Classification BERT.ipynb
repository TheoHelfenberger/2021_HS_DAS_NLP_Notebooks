{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification German News with BERT\n",
    "\n",
    "In this notebook I follow the example given by Joel in \n",
    "[Bert Fine tuning](https://colab.research.google.com/github/AdvancedNLP/encoder/blob/exercise/BERT_doctors_review.ipynb)\n",
    "\n",
    "The example showed a binary classification problem where as the German Newsset is a multiclass classification.\n",
    "Challenge was to get all the Tensorflow stuff correct.\n",
    "\n",
    "---\n",
    "\n",
    "Fine tuning of a pretrained Hugging Face transfomer\n",
    "In this notebook we will be looking at the fine-tuning process of a BERT model that was previously pre-trained on a large german text corpus. We aim at building a classifier to predict doctor ratings from patients' text comments.\n",
    "\n",
    "A detailed description of the German language reviews of doctors by patients 2019 dataset can be found here\n",
    "\n",
    "For the feature creation and the modeling, we will use the Hugging Face implementation of transformers for Tensorflow 2.0. Transformers provides a general architecture implementation for several state of the art models in the natural language domain.\n",
    "\n",
    "NOTE: This notebook and its implementation is heavily influenced by the data-drive Natural Language Processing of German texts blog post\n",
    "\n",
    "\n",
    "### Results F1 Score\n",
    "As a surprise the results (see last column) are even better as the one achieved with the SimpleTransformer model. Which should actually to same but more sophisticated.\n",
    "\n",
    "![classification_results_with_bert_tf.png](classification_results_with_bert_tf.png)\n",
    "\n",
    "\n",
    "Also check https://towardsdatascience.com/multi-class-classification-with-transformers-6cf7b59a033a \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U transformers==4.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'fhnw-nlp-utils>=0.1.6'\n",
    "!pip install pyarrow\n",
    "\n",
    "from fhnw.nlp.utils.storage import load_dataframe\n",
    "from fhnw.nlp.utils.storage import download\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"data/german_news_articles_original_train_and_test_tokenized.parq\"\n",
    "data_all = load_dataframe(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_original</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token_clean</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stem</th>\n",
       "      <th>token_clean_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>US-Außenminister traf Palästinenserpräsident A...</td>\n",
       "      <td>International</td>\n",
       "      <td>test</td>\n",
       "      <td>US Außenminister traf Palästinenserpräsident A...</td>\n",
       "      <td>[us, außenminister, traf, palästinenserpräside...</td>\n",
       "      <td>[us-amerikanischen, außenminister, treffen, pa...</td>\n",
       "      <td>[us, aussenminist, traf, palastinenserprasiden...</td>\n",
       "      <td>[us, außenminister, traf, palästinenserpräside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>Sachwalterschaften werden meist von Verwandten...</td>\n",
       "      <td>Panorama</td>\n",
       "      <td>train</td>\n",
       "      <td>Sachwalterschaften werden meist von Verwandten...</td>\n",
       "      <td>[sachwalterschaften, meist, verwandten, bekann...</td>\n",
       "      <td>[sachwalterschaften, meist, verwenden, bekenne...</td>\n",
       "      <td>[sachwalterschaft, meist, verwandt, bekannt, b...</td>\n",
       "      <td>[sachwalterschaften, meist, verwandten, bekann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>5. August, 10:00 Uhr: Der GameStandard zeigt d...</td>\n",
       "      <td>Web</td>\n",
       "      <td>train</td>\n",
       "      <td>August Uhr Der GameStandard zeigt die Übertra...</td>\n",
       "      <td>[august, uhr, gamestandard, zeigt, übertragung...</td>\n",
       "      <td>[august, uhr, gamestandard, zeigen, übertragun...</td>\n",
       "      <td>[august, uhr, gamestandard, zeigt, ubertrag, l...</td>\n",
       "      <td>[august, uhr, gamestandard, zeigt, übertragung...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_original          label  split  \\\n",
       "579   US-Außenminister traf Palästinenserpräsident A...  International   test   \n",
       "3922  Sachwalterschaften werden meist von Verwandten...       Panorama  train   \n",
       "1199  5. August, 10:00 Uhr: Der GameStandard zeigt d...            Web  train   \n",
       "\n",
       "                                             text_clean  \\\n",
       "579   US Außenminister traf Palästinenserpräsident A...   \n",
       "3922  Sachwalterschaften werden meist von Verwandten...   \n",
       "1199   August Uhr Der GameStandard zeigt die Übertra...   \n",
       "\n",
       "                                            token_clean  \\\n",
       "579   [us, außenminister, traf, palästinenserpräside...   \n",
       "3922  [sachwalterschaften, meist, verwandten, bekann...   \n",
       "1199  [august, uhr, gamestandard, zeigt, übertragung...   \n",
       "\n",
       "                                            token_lemma  \\\n",
       "579   [us-amerikanischen, außenminister, treffen, pa...   \n",
       "3922  [sachwalterschaften, meist, verwenden, bekenne...   \n",
       "1199  [august, uhr, gamestandard, zeigen, übertragun...   \n",
       "\n",
       "                                             token_stem  \\\n",
       "579   [us, aussenminist, traf, palastinenserprasiden...   \n",
       "3922  [sachwalterschaft, meist, verwandt, bekannt, b...   \n",
       "1199  [august, uhr, gamestandard, zeigt, ubertrag, l...   \n",
       "\n",
       "                                  token_clean_stopwords  \n",
       "579   [us, außenminister, traf, palästinenserpräside...  \n",
       "3922  [sachwalterschaften, meist, verwandten, bekann...  \n",
       "1199  [august, uhr, gamestandard, zeigt, übertragung...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to tokenize out news \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes 9\n"
     ]
    }
   ],
   "source": [
    "# As well we must label encode the classes -> Be aware that if not using One-Hot Encoding we well need \n",
    "# sparse_categorical_crossentropy as loss as opposed to  categorical_crossentropy if using One-Hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "data_all.drop(columns=['label_encoded', 'label_onehot'], errors='ignore', inplace=True)\n",
    "data_all['label_encoded'] = encoder.fit_transform(data_all['label'])\n",
    "\n",
    "num_labels = len(encoder.classes_)\n",
    "print(f'Number of classes {num_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_orig = data_all.loc[(data_all[\"split\"] == \"train\")]\n",
    "data_test_orig = data_all.loc[(data_all[\"split\"] == \"test\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with all other models we will not use the test data during the training\n",
    "# This makes f1 scores comparable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids, train_labels, test_labels = train_test_split(\n",
    "    data_train_orig, \n",
    "    data_train_orig[\"label_encoded\"], \n",
    "    random_state=1, \n",
    "    test_size=0.25, \n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_train_orig['token_clean'].apply(len).plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 512\n",
    "\n",
    "def tokenize(review):\n",
    "  encoded = tokenizer.encode_plus(\n",
    "      text=review,\n",
    "      add_special_tokens=True,  # Add `[CLS]` and `[SEP]`\n",
    "      max_length=MAXLEN,  # Max length to truncate/pad\n",
    "      padding='max_length',  # Pad sentence to max length\n",
    "      return_attention_mask=False,  # attention mask not needed for our task\n",
    "      return_token_type_ids=False,\n",
    "      truncation=True, )\n",
    "    \n",
    "  return encoded['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Jähriger fällt wohl bis Saisonende aus Wien Rapid muss wohl bis Saisonende auf Offensivspieler Thomas Murg verzichten Der im Winter aus Ried gekommene Jährige erlitt beim Heimdebakel gegen Admira Wacker Mödling am Samstag einen Teilriss des Innenbandes im linken Knie wie eine Magnetresonanz Untersuchung am Donnerstag ergab Murg erhielt eine Schiene muss aber nicht operiert werden Dennoch steht ihm eine mehrwöchige Pause bevor \n",
      "\n",
      "The first 5 entries of the tokenized string [3, 112, 352, 519, 5483]\n"
     ]
    }
   ],
   "source": [
    "text = data_train_orig['text_clean'].iloc[0]\n",
    "print(text)\n",
    "print(f'\\nThe first 5 entries of the tokenized string {tokenize(text)[0:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6933/6933 [00:38<00:00, 181.27it/s]\n",
      "100%|██████████| 2312/2312 [00:13<00:00, 176.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize train and test data\n",
    "import tqdm\n",
    "train_input_ids = np.array([tokenize(review) for review in tqdm.tqdm(train_ids['text_clean'])])\n",
    "test_input_ids = np.array([tokenize(review) for review in tqdm.tqdm(test_ids['text_clean'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had to reduce BATCH_SIZE to 4 otherwise on my GPU RTX-2060 I was always runnging into OOM\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((train_input_ids, train_labels))\n",
    "                    .shuffle(buffer_size=len(train_input_ids), reshuffle_each_iteration=True)\n",
    "                    .repeat(EPOCHS)\n",
    "                    .batch(BATCH_SIZE))\n",
    "\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices((test_input_ids, test_labels))\n",
    "                    .batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len=MAXLEN):\n",
    "    \"\"\" add multi class classification to pretrained model\n",
    "    \"\"\"\n",
    "\n",
    "    input_word_ids = tf.keras.layers.Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\"\n",
    "    )\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-german-cased\")\n",
    "    encoder_outputs = bert_model(input_word_ids)\n",
    "\n",
    "    pooler_output = encoder_outputs[1]\n",
    "    cls_embedding = pooler_output\n",
    "    \n",
    "    # Need the number of classed for the Dense layer\n",
    "    no_classes = len(encoder.classes_)\n",
    "    stack = tf.keras.layers.Dense(no_classes)(cls_embedding)\n",
    "    # Multi class os use softmax\n",
    "    output = tf.keras.layers.Activation('softmax')(stack)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=input_word_ids, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-german-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-german-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f081c200528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f08221331d8> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f081c200528>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7f08221331d8> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  TFBaseModelOutputWithPool 109081344 \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 9)                 6921      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 109,088,265\n",
      "Trainable params: 109,088,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_len=MAXLEN)\n",
    "# This did not make the training signifcant faster -> do not know why.\n",
    "# So I allowrd to train all params\n",
    "#model.layers[1].trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    # See https://stackoverflow.com/questions/62148508/how-can-i-overcome-valueerror-shapes-none-1-and-none-7-are-incompatible\n",
    "    loss = \"sparse_categorical_crossentropy\" # \"categorical_crossentropy\"\n",
    "    model.compile(optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = compile_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1733/1733 [==============================] - 662s 382ms/step - loss: 0.0539 - accuracy: 0.9838 - val_loss: 0.4908 - val_accuracy: 0.8888\n",
      "\n",
      "Epoch 00001: saving model to training_berts_final/cp.ckpt\n",
      "Epoch 2/2\n",
      "1733/1733 [==============================] - 668s 385ms/step - loss: 0.0432 - accuracy: 0.9886 - val_loss: 0.5253 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00002: saving model to training_berts_final/cp.ckpt\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "checkpoint_path = \"training_berts_final/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "hist = model.fit(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=int(np.floor((len(train_input_ids) / BATCH_SIZE))),\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[cp_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Warning\n",
    "Could not use mode.save_model()\n",
    "It was not possible to load the model again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-german-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-german-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f08069abcf8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint('training_berts_final')\n",
    "model_loaded = build_model(max_len=MAXLEN)\n",
    "model_loaded.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_word_ids (InputLayer)  [(None, 512)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model_2 (TFBertModel TFBaseModelOutputWithPool 109081344 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 6921      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 109,088,265\n",
      "Trainable params: 109,088,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_loaded = compile_model(model_loaded)\n",
    "model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "73/73 - 59s - loss: 0.4633 - accuracy: 0.8798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46330684423446655, 0.8797577619552612]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.evaluate(test_input_ids, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1028/1028 [00:05<00:00, 171.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate tokens of test data\n",
    "import tqdm\n",
    "\n",
    "data_test_orig_ids = np.array([tokenize(review) for review in tqdm.tqdm(data_test_orig['text_clean'])])\n",
    "data_test_orig_labels = data_test_orig[\"label_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 26s 790ms/step - loss: 0.4593 - accuracy: 0.8901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4593358337879181, 0.8900778293609619]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded.evaluate(data_test_orig_ids, data_test_orig_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 - 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.4010879e-03, 3.2960445e-01, 3.5547899e-04, 3.4520729e-04,\n",
       "       1.2724639e-03, 2.7898152e-04, 1.6730170e-04, 6.6632426e-01,\n",
       "       2.5075016e-04], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the predictions for the test data\n",
    "# Will return of each sample an array of len 9 with the probabilities for every class\n",
    "predictions = model.predict(data_test_orig_ids, batch_size=BATCH_SIZE, verbose=2, use_multiprocessing=True)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87        67\n",
      "           1       0.86      0.82      0.84       102\n",
      "           2       0.85      0.90      0.87       151\n",
      "           3       0.86      0.81      0.84        54\n",
      "           4       0.87      0.78      0.82       168\n",
      "           5       0.98      1.00      0.99       120\n",
      "           6       0.94      0.96      0.95       168\n",
      "           7       0.91      0.94      0.92       141\n",
      "           8       0.76      0.89      0.82        57\n",
      "\n",
      "    accuracy                           0.89      1028\n",
      "   macro avg       0.88      0.88      0.88      1028\n",
      "weighted avg       0.89      0.89      0.89      1028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the class with the highest probability\n",
    "most_probable_class = np.argmax(predictions, axis = 1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(data_test_orig_labels, classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to use the inverse transform to get from the integer values to the string classes again\n",
    "# Otherwise we will not be able to compare the f1 score with the other models\n",
    "y_test_pred_inv = encoder.inverse_transform(classes)\n",
    "y_test_inv = encoder.inverse_transform(data_test_orig_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All F1 Score - Maximum highlighted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col9,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col7,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col9,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col2,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col6,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col10,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col10,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col10,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col0,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col10,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col9,#T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col10{\n",
       "            background-color:  lightblue;\n",
       "        }</style><table id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >f1-baseline</th>        <th class=\"col_heading level0 col1\" >f1-one vs one</th>        <th class=\"col_heading level0 col2\" >f1-baseline 2 Gram</th>        <th class=\"col_heading level0 col3\" >f1-kNN</th>        <th class=\"col_heading level0 col4\" >f1-Random Forest</th>        <th class=\"col_heading level0 col5\" >f1-Naiv Bayes</th>        <th class=\"col_heading level0 col6\" >f1-baseline stemming</th>        <th class=\"col_heading level0 col7\" >f1-baseline stemming optimized</th>        <th class=\"col_heading level0 col8\" >f1-dummy clf</th>        <th class=\"col_heading level0 col9\" >f1-simpletransformer</th>        <th class=\"col_heading level0 col10\" >f1-bert-tf-transformer</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row0\" class=\"row_heading level0 row0\" >Etat</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col0\" class=\"data row0 col0\" >0.850394</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col1\" class=\"data row0 col1\" >0.848000</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col2\" class=\"data row0 col2\" >0.806723</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col3\" class=\"data row0 col3\" >0.753846</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col4\" class=\"data row0 col4\" >0.568421</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col5\" class=\"data row0 col5\" >0.029412</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col6\" class=\"data row0 col6\" >0.852713</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col7\" class=\"data row0 col7\" >0.859375</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col8\" class=\"data row0 col8\" >0.031746</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col9\" class=\"data row0 col9\" >0.884058</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row0_col10\" class=\"data row0 col10\" >0.868217</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row1\" class=\"row_heading level0 row1\" >Inland</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col0\" class=\"data row1 col0\" >0.835821</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col1\" class=\"data row1 col1\" >0.827586</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col2\" class=\"data row1 col2\" >0.831683</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col3\" class=\"data row1 col3\" >0.770563</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col4\" class=\"data row1 col4\" >0.780488</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col5\" class=\"data row1 col5\" >0.717647</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col6\" class=\"data row1 col6\" >0.848485</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col7\" class=\"data row1 col7\" >0.869565</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col8\" class=\"data row1 col8\" >0.040201</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col9\" class=\"data row1 col9\" >0.857143</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row1_col10\" class=\"data row1 col10\" >0.840000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row2\" class=\"row_heading level0 row2\" >International</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col0\" class=\"data row2 col0\" >0.855172</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col1\" class=\"data row2 col1\" >0.849123</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col2\" class=\"data row2 col2\" >0.823529</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col3\" class=\"data row2 col3\" >0.813115</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col4\" class=\"data row2 col4\" >0.788274</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col5\" class=\"data row2 col5\" >0.830450</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col6\" class=\"data row2 col6\" >0.851351</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col7\" class=\"data row2 col7\" >0.851351</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col8\" class=\"data row2 col8\" >0.190184</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col9\" class=\"data row2 col9\" >0.876254</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row2_col10\" class=\"data row2 col10\" >0.874598</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row3\" class=\"row_heading level0 row3\" >Kultur</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col0\" class=\"data row3 col0\" >0.854545</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col1\" class=\"data row3 col1\" >0.811321</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col2\" class=\"data row3 col2\" >0.890909</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col3\" class=\"data row3 col3\" >0.742268</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col4\" class=\"data row3 col4\" >0.804124</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col6\" class=\"data row3 col6\" >0.859813</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col7\" class=\"data row3 col7\" >0.878505</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col8\" class=\"data row3 col8\" >0.055556</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col9\" class=\"data row3 col9\" >0.854545</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row3_col10\" class=\"data row3 col10\" >0.838095</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row4\" class=\"row_heading level0 row4\" >Panorama</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col0\" class=\"data row4 col0\" >0.829412</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col1\" class=\"data row4 col1\" >0.802228</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col2\" class=\"data row4 col2\" >0.804598</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col3\" class=\"data row4 col3\" >0.738028</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col4\" class=\"data row4 col4\" >0.717087</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col5\" class=\"data row4 col5\" >0.631791</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col6\" class=\"data row4 col6\" >0.838150</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col7\" class=\"data row4 col7\" >0.826347</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col8\" class=\"data row4 col8\" >0.197015</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col9\" class=\"data row4 col9\" >0.832335</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row4_col10\" class=\"data row4 col10\" >0.821317</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row5\" class=\"row_heading level0 row5\" >Sport</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col0\" class=\"data row5 col0\" >0.991667</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col1\" class=\"data row5 col1\" >0.978903</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col2\" class=\"data row5 col2\" >0.991667</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col3\" class=\"data row5 col3\" >0.962343</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col4\" class=\"data row5 col4\" >0.970464</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col5\" class=\"data row5 col5\" >0.948718</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col6\" class=\"data row5 col6\" >0.987448</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col7\" class=\"data row5 col7\" >0.991667</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col8\" class=\"data row5 col8\" >0.150794</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col9\" class=\"data row5 col9\" >0.978903</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row5_col10\" class=\"data row5 col10\" >0.991736</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row6\" class=\"row_heading level0 row6\" >Web</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col0\" class=\"data row6 col0\" >0.908012</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col1\" class=\"data row6 col1\" >0.900901</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col2\" class=\"data row6 col2\" >0.908012</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col3\" class=\"data row6 col3\" >0.835913</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col4\" class=\"data row6 col4\" >0.850000</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col5\" class=\"data row6 col5\" >0.756219</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col6\" class=\"data row6 col6\" >0.925373</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col7\" class=\"data row6 col7\" >0.923077</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col8\" class=\"data row6 col8\" >0.127796</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col9\" class=\"data row6 col9\" >0.917933</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row6_col10\" class=\"data row6 col10\" >0.947059</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row7\" class=\"row_heading level0 row7\" >Wirtschaft</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col0\" class=\"data row7 col0\" >0.849315</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col1\" class=\"data row7 col1\" >0.813559</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col2\" class=\"data row7 col2\" >0.823129</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col3\" class=\"data row7 col3\" >0.747253</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col4\" class=\"data row7 col4\" >0.719723</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col5\" class=\"data row7 col5\" >0.785965</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col6\" class=\"data row7 col6\" >0.851211</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col7\" class=\"data row7 col7\" >0.844291</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col8\" class=\"data row7 col8\" >0.141343</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col9\" class=\"data row7 col9\" >0.877193</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row7_col10\" class=\"data row7 col10\" >0.923077</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row8\" class=\"row_heading level0 row8\" >Wissenschaft</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col0\" class=\"data row8 col0\" >0.924370</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col1\" class=\"data row8 col1\" >0.902655</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col2\" class=\"data row8 col2\" >0.905983</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col3\" class=\"data row8 col3\" >0.796117</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col4\" class=\"data row8 col4\" >0.844037</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col6\" class=\"data row8 col6\" >0.905983</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col7\" class=\"data row8 col7\" >0.905983</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col8\" class=\"data row8 col8\" >0.052632</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col9\" class=\"data row8 col9\" >0.877193</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row8_col10\" class=\"data row8 col10\" >0.822581</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row9\" class=\"row_heading level0 row9\" >accuracy</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col0\" class=\"data row9 col0\" >0.876459</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col1\" class=\"data row9 col1\" >0.857977</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col2\" class=\"data row9 col2\" >0.861868</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col3\" class=\"data row9 col3\" >0.799611</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col4\" class=\"data row9 col4\" >0.790856</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col5\" class=\"data row9 col5\" >0.694553</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col6\" class=\"data row9 col6\" >0.880350</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col7\" class=\"data row9 col7\" >0.881323</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col8\" class=\"data row9 col8\" >0.131323</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col9\" class=\"data row9 col9\" >0.885214</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row9_col10\" class=\"data row9 col10\" >0.890078</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row10\" class=\"row_heading level0 row10\" >macro avg</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col0\" class=\"data row10 col0\" >0.877634</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col1\" class=\"data row10 col1\" >0.859364</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col2\" class=\"data row10 col2\" >0.865137</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col3\" class=\"data row10 col3\" >0.795494</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col4\" class=\"data row10 col4\" >0.782513</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col5\" class=\"data row10 col5\" >0.522245</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col6\" class=\"data row10 col6\" >0.880059</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col7\" class=\"data row10 col7\" >0.883351</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col8\" class=\"data row10 col8\" >0.109696</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col9\" class=\"data row10 col9\" >0.883951</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row10_col10\" class=\"data row10 col10\" >0.880742</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002level0_row11\" class=\"row_heading level0 row11\" >weighted avg</th>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col0\" class=\"data row11 col0\" >0.876300</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col1\" class=\"data row11 col1\" >0.858965</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col2\" class=\"data row11 col2\" >0.861639</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col3\" class=\"data row11 col3\" >0.800207</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col4\" class=\"data row11 col4\" >0.787416</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col5\" class=\"data row11 col5\" >0.640488</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col6\" class=\"data row11 col6\" >0.880436</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col7\" class=\"data row11 col7\" >0.881183</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col8\" class=\"data row11 col8\" >0.129901</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col9\" class=\"data row11 col9\" >0.885523</td>\n",
       "                        <td id=\"T_d8acf1d8_455e_11ec_8e17_0242ac110002row11_col10\" class=\"data row11 col10\" >0.889405</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f0806ed4f98>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the f1 score to the file with the scores of the baseline models and the experiments\n",
    "import os\n",
    "import class_result\n",
    "\n",
    "filename = 'data/overview_classification_results-with-transformer.csv'\n",
    "df_classification_results = class_result.append_to_classification_report('f1-bert-tf-transformer', y_test_inv, y_test_pred_inv, filename=filename)\n",
    "\n",
    "print(\"All F1 Score - Maximum highlighted\")\n",
    "df_classification_results.style.highlight_max(color = 'lightblue', axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
